{
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Cleaning Data in Python\n\n## Part 3 of 4"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Course Description\n\nA vital component of data science involves acquiring raw data and getting it into a form ready for analysis. It is commonly said that data scientists spend 80% of their time cleaning and manipulating data, and only 20% of their time actually analyzing it. This beginner course will equip you with all the skills you need to clean your data in Python, from learning how to diagnose problems in your data, to dealing with missing values and outliers. At the end of the course, you'll apply all of the techniques you've learned to a case study to clean a real-world Gapminder dataset."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Objectives:\n\n#### 1. Exploring your datasets\n#### 2. Tidying data for analysis\n#### 3. Cleaning data for analysis\n#### 4. Case study\n\n#### 1. Exploring your datasets\nYou have just got a brand new dataset and are itching to start exploring it. But where do you begin, and how can you be sure your dataset is clean? This chapter will introduce you to data cleaning in Python. You'll learn how to explore your data with an eye for diagnosing issues such as outliers, missing values, and duplicate rows\n\n#### 2. Tidying data for analysis\nLearn about the principles of tidy data, and more importantly, why you should care about them and how they make data analysis more efficient. You'll gain first-hand experience with reshaping and tidying data using techniques such as pivoting and melting.\n\n#### 3. Cleaning data for analysis\nDive into some of the grittier aspects of data cleaning. Learn about string manipulation and pattern matching to deal with unstructured data, and then explore techniques to deal with missing or duplicate data. You'll also learn the valuable skill of programmatically checking your data for consistency, which will give you confidence that your code is running correctly and that the results of your analysis are reliable.\n\n#### 4. Case study\nIn this final chapter, you'll apply all of the data cleaning techniques you've learned in this course toward tidying a real-world, messy dataset obtained from the Gapminder Foundation. Once you're done, not only will you have a clean and tidy dataset, you'll also be ready to start working on your own data science projects using Python."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3. Cleaning data for analysis\n\n* 3.1 Data types\n* 3.2 Converting data types\n* 3.3 Working with numeric data\n* 3.4 Using regular expressions to clean strings\n* 3.5 String parsing with regular expressions\n* 3.6 Extracting numerical values from strings\n* 3.7 Pattern matching\n* 3.8 Using functions to clean data\n* 3.9 Custom functions to clean data\n* 3.10 Lambda functions\n* 3.11 Duplicate and missing data\n* 3.12 Dropping duplicate data\n* 3.13 Filling missing data\n* 3.14 Testing with asserts\n* 3.15 Testing your data with asserts"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.1 Data types\n\n##### Watch the video\nhttps://youtu.be/XlRePLtp-Ng\n\n#### 3.2 Converting data types\nIn this exercise, you'll see how ensuring all categorical variables in a DataFrame are of type category reduces memory usage.\n\nThe tips dataset has been loaded into a DataFrame called tips. This data contains information about how much a customer tipped, whether the customer was male or female, a smoker or not, etc.\n\nLook at the output of tips.info() in the IPython Shell. You'll note that two columns that should be categorical - sex and smoker - are instead of type object, which is pandas' way of storing arbitrary strings. Your job is to convert these two columns to type category and note the reduced memory usage.\n\nInstructions\n* Convert the sex column of the tips DataFrame to type 'category' using the .astype() method.\n* Convert the smoker column of the tips DataFrame.\n* Print the memory usage of tips after converting the data types of the columns. Use the .info() method to do this.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Convert the sex column to type 'category'\ntips.sex = tips.sex.____(____)\n\n# Convert the smoker column to type 'category'\ntips.smoker = ____\n\n# Print the info of tips\nprint(____)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.3 Working with numeric data\nIf you expect the data type of a column to be numeric (int or float), but instead it is of type object, this typically means that there is a non numeric value in the column, which also signifies bad data.\n\nYou can use the pd.to_numeric() function to convert a column into a numeric data type. If the function raises an error, you can be sure that there is a bad value within the column. You can either use the techniques you learned in Chapter 1 to do some exploratory data analysis and find the bad value, or you can choose to ignore or coerce the value into a missing value, NaN.\n\nA modified version of the tips dataset has been pre-loaded into a DataFrame called tips. For instructional purposes, it has been pre-processed to introduce some 'bad' data for you to clean. Use the .info() method to explore this. You'll note that the total_bill and tip columns, which should be numeric, are instead of type object. Your job is to fix this.\n\nInstructions:\n* Use pd.to_numeric() to convert the 'total_bill' column of tips to a numeric data type. Coerce the errors to NaN by specifying the keyword argument errors='coerce'.\n* Convert the 'tip' column of 'tips' to a numeric data type exactly as you did for the 'total_bill' column.\n* Print the info of tips to confirm that the data types of 'total_bill' and 'tips' are numeric."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Convert 'total_bill' to a numeric dtype\ntips['total_bill'] = ____(tips[____], errors=____)\n\n# Convert 'tip' to a numeric dtype\ntips['tip'] = ____\n\n# Print the info of tips\nprint(____)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.4 Using regular expressions to clean strings\n\n##### Watch the video\nhttps://youtu.be/97J2xY6yG78\n\n#### 3.5 String parsing with regular expressions\nIn the video, Dan introduced you to the basics of regular expressions, which are powerful ways of defining patterns to match strings. This exercise will get you started with writing them.\n\nWhen working with data, it is sometimes necessary to write a regular expression to look for properly entered values. Phone numbers in a dataset is a common field that needs to be checked for validity. Your job in this exercise is to define a regular expression to match US phone numbers that fit the pattern of xxx-xxx-xxxx.\n\nThe regular expression module in python is re. When performing pattern matching on data, since the pattern will be used for a match across multiple rows, it's better to compile the pattern first using re.compile(), and then use the compiled pattern to match values.\n\nInstructions:\n* Import re.\n* Compile a pattern that matches a phone number of the format xxx-xxx-xxxx.\n* Use \\d{x} to match x digits. Here you'll need to use it three times: twice to match 3 digits, and once to match 4 digits.\n* Place the regular expression inside re.compile().\n* Using the .match() method on prog, check whether the pattern matches the string '123-456-7890'.\n* Using the same approach, now check whether the pattern matches the string '1123-456-7890\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Compile the pattern: prog\nprog = ____('____-____-____')\n\n# See if the pattern matches\nresult = ____('____')\nprint(bool(result))\n\n# See if the pattern matches\nresult2 = ____\nprint(bool(result2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.6 Extracting numerical values from strings\nExtracting numbers from strings is a common task, particularly when working with unstructured data or log files.\n\nSay you have the following string: 'the recipe calls for 6 strawberries and 2 bananas'.\n\nIt would be useful to extract the 6 and the 2 from this string to be saved for later use when comparing strawberry to banana ratios.\n\nWhen using a regular expression to extract multiple numbers (or multiple pattern matches, to be exact), you can use the re.findall() function. Dan did not discuss this in the video, but it is straightforward to use: You pass in a pattern and a string to re.findall(), and it will return a list of the matches.\n\nInstructions:\n* Import re.\n* Write a pattern that will find all the numbers in the following string: 'the recipe calls for 10 strawberries and 1 banana'. To do this:\n* Use the re.findall() function and pass it two arguments: the pattern, followed by the string.\n* \\d is the pattern required to find digits. This should be followed with a + so that the previous element is matched one or more times. This ensures that 10 is viewed as one number and not as 1 and 0.\n* Print the matches to confirm that your regular expression found the values 10 and 1."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import the regular expression module\n____\n\n# Find the numeric values: matches\nmatches = re.findall('____', 'the recipe calls for 10 strawberries and 1 banana')\n\n# Print the matches\nprint(____)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.7 Pattern matching\nIn this exercise, you'll continue practicing your regular expression skills. For each provided string, your job is to write the appropriate pattern to match it.\n\nInstructions:\nWrite patterns to match:\n* A telephone number of the format xxx-xxx-xxxx. You already did this in a previous exercise.\n* A string of the format: A dollar sign, an arbitrary number of digits, a decimal point, 2 digits.\n*     * (a) Use \\$ to match the dollar sign, \\d* to match an arbitrary number of digits, \\. to match the decimal point, and \\d{x} to match x number of digits.\n* A capital letter, followed by an arbitrary number of alphanumeric characters.\n*     * (a) Use [A-Z] to match any capital letter followed by \\w* to match an arbitrary number of alphanumeric characters."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Write the first pattern\npattern1 = bool(re.match(pattern='____', string='123-456-7890'))\nprint(pattern1)\n\n# Write the second pattern\npattern2 = bool(re.match(pattern='____', string='$123.45'))\nprint(pattern2)\n\n# Write the third pattern\npattern3 = bool(re.match(pattern='____', string='Australia'))\nprint(pattern3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.8 Using functions to clean data\n\n##### Watch the video\nhttps://youtu.be/nJNI2XYblyo\n\n#### 3.9 Custom functions to clean data\nYou'll now practice writing functions to clean data.\n\nThe tips dataset has been pre-loaded into a DataFrame called tips. It has a 'sex' column that contains the values 'Male' or 'Female'. Your job is to write a function that will recode 'Female' to 0, 'Male' to 1, and return np.nan for all entries of 'sex' that are neither 'Female' nor 'Male'.\n\nRecoding variables like this is a common data cleaning task. Functions provide a mechanism for you to abstract away complex bits of code as well as reuse code. This makes your code more readable and less error prone.\n\nAs Dan showed you in the videos, you can use the .apply() method to apply a function across entire rows or columns of DataFrames. However, note that each column of a DataFrame is a pandas Series. Functions can also be applied across Series. Here, you will apply your function over the 'sex' column.\n\nInstructions\n* Define a function named recode_gender() that has one parameter: gender.\n* * If gender equals 'Female', return 0.\n* * Else, if gender equals 'Male', return 1.\n* * If gender does not equal 'Male' or 'Female', return np.nan. NumPy has been pre-imported for you.\n* Apply your recode_gender() function over tips.sex using the .apply() method to create a new column: 'recode'. Note that when passing in a function inside the .apply() method, you don't need to specify the parentheses after the function name.\n* Take note of the new 'recode' column in the tips DataFrame!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define recode_gender()\ndef ____:\n\n    # Return 0 if gender is 'Female'\n    if ____ == ____:\n        return ____\n    \n    # Return 1 if gender is 'Male'    \n    elif ____ == ____:\n        return ____\n    \n    # Return np.nan    \n    else:\n        return ____\n\n# Apply the function to the sex column\ntips['recode'] = ____\n\n# Print the first five rows of tips\nprint(tips.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.10 Lambda functions\nYou'll now be introduced to a powerful Python feature that will help you clean your data more effectively: lambda functions. Instead of using the def syntax that you used in the previous exercise, lambda functions let you make simple, one-line functions.\n\nFor example, here's a function that squares a variable used in an .apply() method:\n\ndef my_square(x):\n    return x ** 2\n\ndf.apply(my_square)\n\nThe equivalent code using a lambda function is:\n\ndf.apply(lambda x: x ** 2)\n\nThe lambda function takes one parameter - the variable x. The function itself just squares x and returns the result, which is whatever the one line of code evaluates to. In this way, lambda functions can make your code concise and Pythonic.\n\nThe tips dataset has been pre-loaded into a DataFrame called tips. Your job is to clean its 'total_dollar' column by removing the dollar sign. You'll do this using two different methods: With the .replace() method, and with regular expressions. The regular expression module re has been pre-imported.\n\nInstructions\n* Use the .replace() method inside a lambda function to remove the dollar sign from the 'total_dollar' column of tips.\n* * You need to specify two arguments to the .replace() method: The string to be replaced ('$'), and the string to replace it by ('').\n* * Apply the lambda function over the 'total_dollar' column of tips.\n* Use a regular expression to remove the dollar sign from the 'total_dollar' column of tips.\n* * The pattern has been provided for you: It is the first argument of the re.findall() function.\n* * Complete the rest of the lambda function and apply it over the 'total_dollar' column of tips. Notice that because re.findall() returns a list, you have to slice it (e.g. using [0]) in order to access the actual value.\n* Verify that you have removed the dollar sign from the column."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Write the lambda function using replace\ntips['total_dollar_replace'] = tips.total_dollar.apply(____ x: x.____('$', ''))\n\n# Write the lambda function using regular expressions\ntips['total_dollar_re'] = tips.total_dollar.apply(____ ____: ____('\\d+\\.\\d+', x)[0])\n\n# Print the head of tips\nprint(tips.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.11 Duplicate and missing data\n\n##### Watch the video\nhttps://youtu.be/61Xz8DROah0\n\n#### 3.12 Dropping duplicate data\nDuplicate data causes a variety of problems. From the point of view of performance, they use up unnecessary amounts of memory and cause unneeded calculations to be performed when processing data. In addition, they can also bias any analysis results.\n\nA dataset consisting of the performance of songs on the Billboard charts has been pre-loaded into a DataFrame called billboard. Check out its columns in the IPython Shell. Your job in this exercise is to subset this DataFrame and then drop all duplicate rows.\n\nInstructions\n* Create a new DataFrame called tracks that contains the following columns from billboard: 'year', 'artist', 'track', and 'time'.\n* Print the info of tracks. This has been done for you.\n* Drop duplicate rows from tracks using the .drop_duplicates() method. Save the result to tracks_no_duplicates.\n* Print the info of tracks_no_duplicates. This has been done for you, so hit 'Run' to see the results!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create the new DataFrame: tracks\ntracks = billboard[[____]]\n\n# Print info of tracks\nprint(tracks.info())\n\n# Drop the duplicates: tracks_no_duplicates\ntracks_no_duplicates = ____\n\n# Print info of tracks\nprint(tracks_no_duplicates.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.13 Filling missing data\nHere, you'll return to the airquality dataset from Chapter 2. It has been pre-loaded into the DataFrame airquality, and it has missing values for you to practice filling in. Explore airquality in the IPython Shell to checkout which columns have missing values.\n\nIt's rare to have a (real-world) dataset without any missing values, and it's important to deal with them because certain calculations cannot handle missing values while some calculations will, by default, skip over any missing values.\n\nAlso, understanding how much missing data you have, and thinking about where it comes from is crucial to making unbiased interpretations of data.\n\nInstructions\n* Calculate the mean of the Ozone column of airquality using the .mean() method on airquality.Ozone.\n* Use the .fillna() method to replace all the missing values in the Ozone column of airquality with the mean, oz_mean.\n* Hit 'Run' to see the result of filling in the missing values!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Calculate the mean of the Ozone column: oz_mean\noz_mean = ____\n\n# Replace all the missing values in the Ozone column with the mean\nairquality['Ozone'] = ____\n\n# Print the info of airquality\nprint(airquality.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 3.14 Testing with asserts\n\n##### Watch the video\nhttps://youtu.be/8X8UwWBTDjg\n\n#### 3.15 Testing your data with asserts\nHere, you'll practice writing assert statements using the Ebola dataset from previous chapters to programmatically check for missing values and to confirm that all values are positive. The dataset has been pre-loaded into a DataFrame called ebola.\n\nIn the video, you saw Dan use the .all() method together with the .notnull() DataFrame method to check for missing values in a column. The .all() method returns True if all values are True. When used on a DataFrame, it returns a Series of Booleans - one for each column in the DataFrame. So if you are using it on a DataFrame, like in this exercise, you need to chain another .all() method so that you return only one True or False value. When using these within an assert statement, nothing will be returned if the assert statement is true: This is how you can confirm that the data you are checking are valid.\n\nNote: You can use pd.notnull(df) as an alternative to df.notnull().\n\nInstructions\n* Write an assert statement to confirm that there are no missing values in ebola.\n* * Use the pd.notnull() function on ebola (or the .notnull() method of ebola) and chain two .all() methods (that is, .all().all()). The first .all() method will return a True or False for each column, while the second .all() method will return a single True or False.\n* Write an assert statement to confirm that all values in ebola are greater than or equal to 0.\n* * Chain two all() methods to the Boolean condition (ebola >= 0)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Assert that there are no missing values\nassert ____\n\n# Assert that all values are >= 0\nassert (ebola >= 0).____",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### End of Lesson 3 - Cleaning Data for Analysis\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}