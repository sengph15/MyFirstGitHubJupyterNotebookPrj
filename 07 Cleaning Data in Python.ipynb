{
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Cleaning Data in Python\n\n## Part 4 of 4"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Course Description\n\nA vital component of data science involves acquiring raw data and getting it into a form ready for analysis. It is commonly said that data scientists spend 80% of their time cleaning and manipulating data, and only 20% of their time actually analyzing it. This beginner course will equip you with all the skills you need to clean your data in Python, from learning how to diagnose problems in your data, to dealing with missing values and outliers. At the end of the course, you'll apply all of the techniques you've learned to a case study to clean a real-world Gapminder dataset."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Objectives:\n\n#### 1. Exploring your datasets\n#### 2. Tidying data for analysis\n#### 3. Cleaning data for analysis\n#### 4. Case study\n\n#### 1. Exploring your datasets\nYou have just got a brand new dataset and are itching to start exploring it. But where do you begin, and how can you be sure your dataset is clean? This chapter will introduce you to data cleaning in Python. You'll learn how to explore your data with an eye for diagnosing issues such as outliers, missing values, and duplicate rows\n\n#### 2. Tidying data for analysis\nLearn about the principles of tidy data, and more importantly, why you should care about them and how they make data analysis more efficient. You'll gain first-hand experience with reshaping and tidying data using techniques such as pivoting and melting.\n\n#### 3. Cleaning data for analysis\nDive into some of the grittier aspects of data cleaning. Learn about string manipulation and pattern matching to deal with unstructured data, and then explore techniques to deal with missing or duplicate data. You'll also learn the valuable skill of programmatically checking your data for consistency, which will give you confidence that your code is running correctly and that the results of your analysis are reliable.\n\n#### 4. Case study\nIn this final chapter, you'll apply all of the data cleaning techniques you've learned in this course toward tidying a real-world, messy dataset obtained from the Gapminder Foundation. Once you're done, not only will you have a clean and tidy dataset, you'll also be ready to start working on your own data science projects using Python."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4. Case study\n\n* 4.1 Putting it all together\n* 4.2 Exploratory analysis\n* 4.3 Visualizing your data\n* 4.4 Thinking about the question at hand\n* 4.5 Assembling your data\n* 4.6 Initial impressions of the data\n* 4.7 Reshaping your data\n* 4.8 Checking the data types\n* 4.9 Looking at country spellings\n* 4.10 More data cleaning and processing\n* 4.11 Wrapping up\n* 4.12 Final thoughts"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 4.1 Putting it all together\n\n##### Watch the video\nhttps://youtu.be/NpaAAW6H9fM\n\n#### 4.2 Exploratory analysis\nWhenever you obtain a new dataset, your first task should always be to do some exploratory analysis to get a better understanding of the data and diagnose it for any potential issues.\n\nThe Gapminder data for the 19th century has been loaded into a DataFrame called g1800s. In the IPython Shell, use pandas methods such as .head(), .info(), and .describe(), and DataFrame attributes like .columns and .shape to explore it.\n\nUse the information that you acquire from your exploratory analysis to choose the true statement from the options provided below.\n\nPossible Answers\n* (a) The DataFrame has 259 rows and 100 columns.\n* (b) The DataFrame has no missing values encoded as NaN.\n* (c) 100 of the columns are of type float64 and 1 column is of type object.\n* (d) The DataFrame takes up 203.2+ KB of memory.\n\nUse the command line to check your answer\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 4.3 Possible Answers\nSince 1800, life expectancy around the globe has been steadily going up. You would expect the Gapminder data to confirm this.\n\nThe DataFrame g1800s has been pre-loaded. Your job in this exercise is to create a scatter plot with life expectancy in '1800' on the x-axis and life expectancy in '1899' on the y-axis.\n\nHere, the goal is to visually check the data for insights as well as errors. When looking at the plot, pay attention to whether the scatter plot takes the form of a diagonal line, and which points fall below or above the diagonal line. This will inform how life expectancy in 1899 changed (or did not change) compared to 1800 for different countries. If points fall on a diagonal line, it means that life expectancy remained the same!\n\nInstructions:\n* Import matplotlib.pyplot as plt.\n* Use the .plot() method on g1800s with kind='scatter' to create a scatter plot with '1800' on the x-axis and '1899' on the y-axis.\n* Display the plot."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import matplotlib.pyplot\n____\n\n# Create the scatter plot\n____(kind=____, x=____, y=____)\n\n# Specify axis labels\nplt.xlabel('Life Expectancy by Country in 1800')\nplt.ylabel('Life Expectancy by Country in 1899')\n\n# Specify axis limits\nplt.xlim(20, 55)\nplt.ylim(20, 55)\n\n# Display the plot\n____",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 4.4 Thinking about the question at hand\n\nSince you are given life expectancy level data by country and year, you could ask questions about how much the average life expectancy changes over each year.\n\nBefore continuing, however, it's important to make sure that the following assumptions about the data are true:\n\n'Life expectancy' is the first column (index 0) of the DataFrame.\nThe other columns contain either null or numeric values.\nThe numeric values are all greater than or equal to 0.\nThere is only one instance of each country.\nYou can write a function that you can apply over the entire DataFrame to verify some of these assumptions. Note that spending the time to write such a script will help you when working with other datasets as well.\n\nInstructions:\n* Define a function called check_null_or_valid() that takes in one argument: row_data.\n* Inside the function, convert no_na to a numeric data type using pd.to_numeric().\n* Write an assert statement to make sure the first column (index 0) of the g1800s DataFrame is 'Life expectancy'.\n* Write an assert statement to test that all the values are valid for the g1800s DataFrame. Use the check_null_or_valid() function placed inside the .apply() method for this.\n* * Note that because you're applying it over the entire DataFrame, and not just one column, you'll have to chain the .all() method twice.\n* * Remember that you don't have to use () for functions placed inside .apply().\n* Write an assert statement to make sure that each country occurs only once in the data. Use the .value_counts() method on the 'Life expectancy' column for this. Specifically, index 0 of .value_counts() will contain the most frequently occurring value. If this is equal to 1 for the 'Life expectancy' column, then you can be certain that no country appears more than once in the data.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def check_null_or_valid(row_data):\n    \"\"\"Function that takes a row of data,\n    drops all missing values,\n    and checks if all remaining values are greater than or equal to 0\n    \"\"\"\n    no_na = row_data.dropna()\n    numeric = ____\n    ge0 = numeric >= 0\n    return ge0\n\n# Check whether the first column is 'Life expectancy'\n____ g1800s.columns[____] == ____\n\n# Check whether the values in the row are valid\n____ g1800s.iloc[:, 1:].apply(____, axis=1).all().all()\n\n# Check that there is only one instance of each country\n____ g1800s['____'].value_counts()[0] == ____",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 4.5 Assembling your data\nHere, three DataFrames have been pre-loaded: g1800s, g1900s, and g2000s. These contain the Gapminder life expectancy data for, respectively, the 19th century, the 20th century, and the 21st century.\n\nYour task in this exercise is to concatenate them into a single DataFrame called gapminder. This is a column-wise concatenation. \n\nInstructions:\n* Use pd.concat() to concatenate g1800s, g1900s, and g2000s along the column axis into one DataFrame called gapminder. Make sure you pass DataFrames to pd.concat() in the form of a list.\n* Print the shape and the head of the concatenated DataFrame."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Concatenate the DataFrames column-wise\ngapminder = pd.concat([g1800s, g1900s, ______], axis='columns')\n\n# Print the shape of gapminder\nprint(_____)\n\n# Print the head of gapminder\nprint(______)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 4.6 Initial impressions of the data\n\n##### Watch the video\nhttps://youtu.be/wiqExSjg4LY\n\n#### 4.7Reshaping your data\nNow that you have all the data combined into a single DataFrame, the next step is to reshape it into a tidy data format.\n\nCurrently, the gapminder DataFrame has a separate column for each year. What you want instead is a single column that contains the year, and a single column that represents the average life expectancy for each year and country. By having year in its own column, you can use it as a predictor variable in a later analysis.\n\nYou can convert the DataFrame into the desired tidy format by melting it.\n\nInstructions:\n* Import pandas\n* Reshape gapminder by melting it. Keep 'Life expectancy' fixed by specifying it as an argument to the id_vars parameter.\n* Rename the three columns of the melted DataFrame to 'country', 'year', and 'life_expectancy' by passing them in as a list to gapminder_melt.columns.\n* Print the head of the melted DataFrame."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\n# Melt gapminder: gapminder_melt\ngapminder_melt = pd.melt(gapminder, id_vars='_____')\n\n# Rename the columns\ngapminder_melt.columns = ['country', _____, _____]\n\n# Print the head of gapminder_melt\nprint(_____)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 4.8 Checking the data types\n\nNow that your data are in the proper shape, you need to ensure that the columns are of the proper data type. That is, you need to ensure that country is of type object, year is of type int64, and life_expectancy is of type float64.\n\nThe tidy DataFrame has been pre-loaded as gapminder. Explore it in the IPython Shell using the .info() method. Notice that the column 'year' is of type object. This is incorrect, so you'll need to use the pd.to_numeric() function to convert it to a numeric data type.\n\nImport NumPy and pandas. \n\nInstructions\n* Import NumPy\n* Import pandas\n* Convert the year column of gapminder using pd.to_numeric().\n* Assert that the country column is of type np.object. This has been done for you.\n* Assert that the year column is of type np.int64.\n* Assert that the life_expectancy column is of type np.float64."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Convert the year column to numeric\ngapminder.year = pd.to_numeric(_____)\n\n# Test if country is of type object\nassert gapminder.country.dtypes == _____\n\n# Test if year is of type int64\nassert gapminder.year.dtypes == _____\n\n# Test if life_expectancy is of type float64\n______ gapminder._____ == _____",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 4.9 Looking at country spellings\nHaving tidied your DataFrame and checked the data types, your next task in the data cleaning process is to look at the 'country' column to see if there are any special or invalid characters you may need to deal with.\n\nIt is reasonable to assume that country names will contain:\n\nThe set of lower and upper case letters.\nWhitespace between words.\nPeriods for any abbreviations.\nTo confirm that this is the case, you can leverage the power of regular expressions again. For common operations like this, Pandas has a built-in string method - str.contains() - which takes a regular expression pattern, and applies it to the Series, returning True if there is a match, and False otherwise.\n\nSince here you want to find the values that do not match, you have to invert the boolean, which can be done using ~. This Boolean series can then be used to get the Series of countries that have invalid names.\n\nInstructions\n* Create a Series called countries consisting of the 'country' column of gapminder.\n* Drop all duplicates from countries using the .drop_duplicates() method.\n* Write a regular expression that tests your assumptions of what characters belong in countries:\n* * Anchor the pattern to match exactly what you want by placing a ^ in the beginning and $ in the end.\n* * Use A-Za-z to match the set of lower and upper case letters, \\. to match periods, and \\s to match whitespace between words.\n* Use str.contains() to create a Boolean vector representing values that match the pattern.\n* Invert the mask by placing a ~ before it.\n* Subset the countries series using the .loc[] accessor and mask_inverse. Then hit 'Submit Answer' to see the invalid country names!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create the series of countries: countries\ncountries = ____\n\n# Drop all the duplicates from countries\ncountries = countries.____\n\n# Write the regular expression: pattern\npattern = '^[____]*$'\n\n# Create the Boolean vector: mask\nmask = countries.str.contains(____)\n\n# Invert the mask: mask_inverse\nmask_inverse = ____\n\n# Subset countries using mask_inverse: invalid_countries\ninvalid_countries = ____\n\n# Print invalid_countries\nprint(invalid_countries)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 4.10 More data cleaning and processing\n\nIt's now time to deal with the missing data. There are several strategies for this: You can drop them, fill them in using the mean of the column or row that the missing value is in (also known as imputation), or, if you are dealing with time series data, use a forward fill or backward fill, in which you replace missing values in a column with the most recent known value in the column. See pandas Foundations for more on forward fill and backward fill.\n\nIn general, it is not the best idea to drop missing values, because in doing so you may end up throwing away useful information. In this data, the missing values refer to years where no estimate for life expectancy is available for a given country. You could fill in, or guess what these life expectancies could be by looking at the average life expectancies for other countries in that year, for example. Whichever strategy you go with, it is important to carefully consider all options and understand how they will affect your data.\n\nIn this exercise, you'll practice dropping missing values. Your job is to drop all the rows that have NaN in the life_expectancy column. Before doing so, it would be valuable to use assert statements to confirm that year and country do not have any missing values.\n\nBegin by printing the shape of gapminder in the IPython Shell prior to dropping the missing values. Complete the exercise to find out what its shape will be after dropping the missing values!\n\nInstructions\n* Assert that country and year do not contain any missing values. The first assert statement has been written for you. Note the chaining of the .all() method to pd.notnull() to confirm that all values in the column are not null.\n* Drop the rows in the data where any observation in life_expectancy is missing. As you confirmed that country and year don't have missing values, you can use the .dropna() method on the entire gapminder DataFrame, because any missing values would have to be in the life_expectancy column. The .dropna() method has the default keyword arguments axis=0 and how='any', which specify that rows with any missing values should be dropped.\n* Print the shape of gapminder."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Assert that country does not contain any missing values\nassert pd.notnull(gapminder.country).all()\n\n# Assert that year does not contain any missing values\n____\n\n# Drop the missing values\ngapminder = ____\n\n# Print the shape of gapminder\nprint(____)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 4.11 Wrapping up\nNow that you have a clean and tidy dataset, you can do a bit of visualization and aggregation. In this exercise, you'll begin by creating a histogram of the life_expectancy column. You should not get any values under 0 and you should see something reasonable on the higher end of the life_expectancy age range.\n\nYour next task is to investigate how average life expectancy changed over the years. To do this, you need to subset the data by each year, get the life_expectancy column from each subset, and take an average of the values. You can achieve this using the .groupby() method. This .groupby() method is covered in greater depth in Manipulating DataFrames with pandas.\n\nFinally, you can save your tidy and summarized DataFrame to a file using the .to_csv() method.\n\nmatplotlib.pyplot and pandas have been pre-imported as plt and pd. Go for it!\n\nInstructions\n* Create a histogram of the life_expectancy column using the .plot() method of gapminder. Specify kind='hist'.\n* Group gapminder by 'year' and aggregate 'life_expectancy' by the mean. To do this:\n* * Use the .groupby() method on gapminder with 'year' as the argument. Then select 'life_expectancy' and chain the .mean() method to it.\n* Print the head and tail of gapminder_agg. This has been done for you.\n* Create a line plot of average life expectancy per year by using the .plot() method (without any arguments in plot) on gapminder_agg.\n* Save gapminder and gapminder_agg to csv files called 'gapminder.csv' and 'gapminder_agg.csv', respectively, using the .to_csv() method."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Add first subplot\nplt.subplot(2, 1, 1) \n\n# Create a histogram of life_expectancy\n____\n\n# Group gapminder: gapminder_agg\ngapminder_agg = ____.____('____')['____'].____\n\n# Print the head of gapminder_agg\nprint(gapminder_agg.head())\n\n# Print the tail of gapminder_agg\nprint(gapminder_agg.tail())\n\n# Add second subplot\nplt.subplot(2, 1, 2)\n\n# Create a line plot of life expectancy per year\n____\n\n# Add title and specify axis labels\nplt.title('Life expectancy over the years')\nplt.ylabel('Life expectancy')\nplt.xlabel('Year')\n\n# Display the plots\nplt.tight_layout()\nplt.show()\n\n# Save both DataFrames to csv files\n____\n____",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 4.12 Final thoughts\n\n##### Watch the video\nhttps://youtu.be/3mC7qmD2d2U\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### End of Lesson 3 - Cleaning Data for Analysis\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}