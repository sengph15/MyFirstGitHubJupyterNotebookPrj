{
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Cleaning Data in Python\n\n## Part 1 of 4"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Course Description\n\nA vital component of data science involves acquiring raw data and getting it into a form ready for analysis. It is commonly said that data scientists spend 80% of their time cleaning and manipulating data, and only 20% of their time actually analyzing it. This beginner course will equip you with all the skills you need to clean your data in Python, from learning how to diagnose problems in your data, to dealing with missing values and outliers. At the end of the course, you'll apply all of the techniques you've learned to a case study to clean a real-world Gapminder dataset."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Objectives:\n\n#### 1. Exploring your datasets\n#### 2. Tidying data for analysis\n#### 3. Cleaning data for analysis\n#### 4. Case study\n\n#### 1. Exploring your datasets\nYou have just got a brand new dataset and are itching to start exploring it. But where do you begin, and how can you be sure your dataset is clean? This chapter will introduce you to data cleaning in Python. You'll learn how to explore your data with an eye for diagnosing issues such as outliers, missing values, and duplicate rows\n\n#### 2. Tidying data for analysis\nLearn about the principles of tidy data, and more importantly, why you should care about them and how they make data analysis more efficient. You'll gain first-hand experience with reshaping and tidying data using techniques such as pivoting and melting.\n\n#### 3. Cleaning data for analysis\nDive into some of the grittier aspects of data cleaning. Learn about string manipulation and pattern matching to deal with unstructured data, and then explore techniques to deal with missing or duplicate data. You'll also learn the valuable skill of programmatically checking your data for consistency, which will give you confidence that your code is running correctly and that the results of your analysis are reliable.\n\n#### 4. Case study\nIn this final chapter, you'll apply all of the data cleaning techniques you've learned in this course toward tidying a real-world, messy dataset obtained from the Gapminder Foundation. Once you're done, not only will you have a clean and tidy dataset, you'll also be ready to start working on your own data science projects using Python."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1. Exploring your datasets\n\n* 1.1 Diagnose data for cleaning\n* 1.2 Loading and viewing your data\n* 1.3 Further diagnosis\n* 1.4 Exploratory data analysis\n* 1.5 Calculating summary statistics\n* 1.6 Frequency counts for categorical data\n* 1.7 Visual exploratory data analysis\n* 1.8 Visualizing single variables with histograms\n* 1.9 Visualizing multiple variables with boxplots\n* 1.10 Visualizing multiple variables with scatter plots\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 1.1 Diagnose Data for cleaning\n\n##### Watch the video\nhttps://youtu.be/fCnOmJIejkI\n\n#### 1.2 Loading and viewing your data\nIn this chapter, you're going to look at a subset of the Department of Buildings Job Application Filings dataset from the NYC Open Data portal. This dataset consists of job applications filed on January 22, 2017.\n\nYour first task is to load this dataset into a DataFrame and then inspect it using the .head() and .tail() methods. However, you'll find out very quickly that the printed results don't allow you to see everything you need, since there are too many columns. Therefore, you need to look at the data in another way.\n\nThe .shape and .columns attributes let you see the shape of the DataFrame and obtain a list of its columns. From here, you can see which columns are relevant to the questions you'd like to ask of the data. To this end, a new DataFrame, df_subset, consisting only of these relevant columns, has been pre-loaded. This is the DataFrame you'll work with in the rest of the chapter.\n\nGet acquainted with the dataset now by exploring it with pandas! This initial exploratory analysis is a crucial first step of data cleaning.\n\n* Import pandas as pd.\n* Read 'dob_job_application_filings_subset.csv' into a DataFrame called df.\n* Print the head and tail of df.\n* Print the shape of df and its columns. Note: .shape and .columns are attributes, not methods, so you don't need to follow these with parentheses ().\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import pandas\n____\n\n# Read the file into a DataFrame: df\ndf = ____\n\n# Print the head of df\nprint(____)\n\n# Print the tail of df\nprint(____)\n\n# Print the shape of df\nprint(____)\n\n# Print the columns of df\nprint(____)\n\n# Print the head and tail of df_subset\nprint(df_subset.head())\nprint(df_subset.tail())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 1.3 Further diagnosis\nIn the previous exercise, you identified some potentially unclean or missing data. Now, you'll continue to diagnose your data with the very useful .info() method.\n\nThe .info() method provides important information about a DataFrame, such as the number of rows, number of columns, number of non-missing values in each column, and the data type stored in each column. This is the kind of information that will allow you to confirm whether the 'Initial Cost' and 'Total Est. Fee' columns are numeric or strings. From the results, you'll also be able to see whether or not all columns have complete data in them.\n\nThe full DataFrame df and the subset DataFrame df_subset have been pre-loaded. Your task is to use the .info() method on these and analyze the results.\n\nInstructions:\n* Print the info of df.\n* Print the info of the subset dataframe, df_subset."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Print the info of df\nprint(____)\n\n# Print the info of df_subset\nprint(____)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 1.4 Exploratory Data Analysis\n\n##### Watch the video\nhttps://youtu.be/fCnOmJIejkI\n\n#### 1.5 Calculating summary statistics\nYou'll now use the .describe() method to calculate summary statistics of your data.\n\nIn this exercise, an adapted DataFrame has been prepared for you to inspect, with fewer columns to increase readability in the IPython Shell.\n\nThis adapted DataFrame has been pre-loaded as df. Your job is to use the .describe() method on it in the IPython Shell and select the statement below that is False.\n\nPossible Answers\n* (a) The mean of 'Existing Height' is 94.022809.\n* (b) There are 12846 entries in the DataFrame.\n* (c) The standard deviation of 'Street Frontage' is 11.874080.\n* (d) The maximum of 'Proposed Height' is 4200.\n\nUse the command line to get the answer"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 1.6 Frequency counts for categorical data\nAs you've seen, .describe() can only be used on numeric columns. So how can you diagnose data issues when you have categorical data? One way is by using the .value_counts() method, which returns the frequency counts for each unique value in a column!\n\nThis method also has an optional parameter called dropna which is True by default. What this means is if you have missing data in a column, it will not give a frequency count of them. You want to set the dropna column to False so if there are missing values in a column, it will give you the frequency counts.\n\nIn this exercise, you're going to look at the 'Borough', 'State', and 'Site Fill' columns to make sure all the values in there are valid. When looking at the output, do a sanity check: Are all values in the 'State' column from NY, for example? Since the dataset consists of applications filed in NY, you would expect this to be the case.\n\nInstructions:\n* Print the value counts for:\n* (a) The 'Borough' column.\n* (b) The 'State' column.\n* (c) The 'Site Fill' column."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Print the value counts for 'Borough'\nprint(df['____'].____(dropna=False))\n\n# Print the value_counts for 'State'\nprint(____)\n\n# Print the value counts for 'Site Fill'\nprint(____)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 1.7 Visual exploratory data analysis\n\n##### Watch the video\nhttps://youtu.be/U_QaFbr_Erc\n\n#### 1.8 Visualizing single variables with histograms\nUp until now, you've been looking at descriptive statistics of your data. One of the best ways to confirm what the numbers are telling you is to plot and visualize the data.\n\nYou'll start by visualizing single variables using a histogram for numeric values. The column you will work on in this exercise is 'Existing Zoning Sqft'.\n\nThe .plot() method allows you to create a plot of each column of a DataFrame. The kind parameter allows you to specify the type of plot to use - kind='hist', for example, plots a histogram.\n\nIn the IPython Shell, begin by computing summary statistics for the 'Existing Zoning Sqft' column using the .describe() method. You'll notice that there are extremely large differences between the min and max values, and the plot will need to be adjusted accordingly. In such cases, it's good to look at the plot on a log scale. The keyword arguments logx=True or logy=True can be passed in to .plot() depending on which axis you want to rescale.\n\nFinally, note that Python will render a plot such that the axis will hold all the information. That is, if you end up with large amounts of whitespace in your plot, it indicates counts or values too small to render.\n\nInstructions\n* Import matplotlib.pyplot as plt.\n* Create a histogram of the 'Existing Zoning Sqft' column. Rotate the axis labels by 70 degrees and use a log scale for both axes.\n* Display the histogram using plt.show()."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import matplotlib.pyplot\n____\n\n# Describe the column\nprint(df['Existing Zoning Sqft'].describe())\n\n# Plot the histogram\ndf['____'].____(kind='____', rot=70, logx=____, logy=True)\n\n# Display the histogram\n____",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 1.9 Visualizing multiple variables with boxplots\n\nHistograms are great ways of visualizing single variables. To visualize multiple variables, boxplots are useful, especially when one of the variables is categorical.\n\nIn this exercise, your job is to use a boxplot to compare the 'initial_cost' across the different values of the 'Borough' column. The pandas .boxplot() method is a quick way to do this, in which you have to specify the column and by parameters. Here, you want to visualize how 'initial_cost' varies by 'Borough'.\n\npandas and matplotlib.pyplot have been imported for you as pd and plt, respectively, and the DataFrame has been pre-loaded as df.\n\nInstructions\n* Using the .boxplot() method of df, create a boxplot of 'initial_cost' across the different values of 'Borough'.\n* Display the plot."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import necessary modules\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Create the boxplot\ndf.____(column=____, by=____, rot=90)\n\n# Display the plot\n____",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 1.10 Visualizing multiple variables with scatter plots\n\nBoxplots are great when you have a numeric column that you want to compare across different categories. When you want to visualize two numeric columns, scatter plots are ideal.\n\nIn this exercise, your job is to make a scatter plot with 'initial_cost' on the x-axis and the 'total_est_fee' on the y-axis. You can do this by using the DataFrame .plot() method with kind='scatter'. You'll notice right away that there are 2 major outliers shown in the plots.\n\nSince these outliers dominate the plot, an additional DataFrame, df_subset, has been provided, in which some of the extreme values have been removed. After making a scatter plot using this, you'll find some interesting patterns here that would not have been seen by looking at summary statistics or 1 variable plots.\n\nWhen you're done, you can cycle between the two plots by clicking the 'Previous Plot' and 'Next Plot' buttons below the plot.\n\nInstructions\n* Using df, create a scatter plot (kind='scatter') with 'initial_cost' on the x-axis and the 'total_est_fee' on the y-axis. Rotate the x-axis labels by 70 degrees.\n* Create another scatter plot exactly as above, substituting df_subset in place of df."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import necessary modules\nimport pandas as pd\n# Import necessary modules\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Create and display the first scatter plot\ndf.plot(kind=____, x=____, y=____, rot=70)\nplt.show()\n\n# Create and display the second scatter plot\n____\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### End of Lesson 1 - Exploring Your Datasets\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}